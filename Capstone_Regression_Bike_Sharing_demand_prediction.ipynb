{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "KSlN3yHqYklG",
        "-JiQyfWJYklI",
        "xiyOF9F70UgQ",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "h_CCil-SKHpo",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruksz/Bike-Sharing-Demand-Prediction-Capstone/blob/main/Capstone_Regression_Bike_Sharing_demand_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Bike Sharing Demand Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Rukshar Shaikh\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Bike Sharing Demand Prediction\" project is aimed at helping bike-sharing companies effectively manage their resources and provide a better service to customers. It uses historical data and machine learning to predict the number of bike rentals on any given day. By doing so, it ensures that the right number of bikes are available where and when they are needed, thus maximizing customer satisfaction and operational efficiency.\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ruksz/Bike-Sharing-Demand-Prediction-Capstone.git"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This \"Bike Sharing Demand Prediction\" project aims to help bike rental companies deal with the challenge of knowing how many bikes they need at any given time. It's tricky for these companies because the number of people renting bikes can change a lot from one moment to the next. If they have too few bikes, customers get frustrated because there aren't enough to go around. But if they have too many bikes, it's a waste of money because those extra bikes aren't getting used.\n",
        "\n",
        "So, our project is all about using computer programs to predict how many bikes will be needed. This prediction will help bike rental companies make sure they have just the right number of bikes available, so customers are happy, and the companies save money. It's a win-win situation for everyone!"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "\n",
        "#ignore  the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('always')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#libraries used to pre-process\n",
        "from sklearn import preprocessing, linear_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "#libraries used to implement models\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV,cross_val_score\n",
        "\n",
        "#libraries to evaluate performance\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, mean_absolute_error\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "data = pd.read_csv('/content/drive/MyDrive/MLProject/SeoulBikeData.csv', encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "data.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(data.isnull(), cbar=False)\n",
        "plt.title('Null Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(5)"
      ],
      "metadata": {
        "id": "e_FV_e51vp6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date: The date of the recorded data.\n",
        "\n",
        "Rented Bike Count: The number of bikes rented or used on that date.\n",
        "\n",
        "Hour: The hour of the day when the data was recorded.\n",
        "\n",
        "Temperature (°C): The temperature in degrees Celsius.\n",
        "\n",
        "Humidity (%): The relative humidity as a percentage.\n",
        "\n",
        "Wind Speed (m/s): The wind speed in meters per second.\n",
        "\n",
        "Visibility (10m): The visibility in meters.\n",
        "\n",
        "Dew Point Temperature (°C): The dew point temperature in degrees Celsius.\n",
        "\n",
        "Solar Radiation (MJ/m2): The solar radiation measured in mega-joules per square meter.\n",
        "\n",
        "Rainfall (mm): The amount of rainfall in millimeters.\n",
        "\n",
        "Snowfall (cm): The amount of snowfall in centimeters.\n",
        "\n",
        "Seasons: The season when the data was recorded (e.g., Winter, Spring).\n",
        "\n",
        "Holiday: Whether it was a holiday on that date (e.g., No Holiday).\n",
        "\n",
        "Functioning Day: Indicates whether it was a functioning day (e.g., Yes).\n",
        "\n",
        "This dataset seems to be suitable for time-series analysis and regression tasks, as it contains both temporal and weather-related features that can be used to predict the demand for rented bikes."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in data.columns.tolist():\n",
        "  print(\"Unique values in \",i,\"is: \",data[i].nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "df=data.copy()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting date column dtype object to date\n",
        "df['Date']=pd.to_datetime(df['Date'])"
      ],
      "metadata": {
        "id": "9p5j9oYqzFsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split day of week, month and year in three column\n",
        "df['day_of_week'] = df['Date'].dt.day_name() # extract week name from Date column\n",
        "df[\"month\"] = df['Date'].dt.month_name()   # extract month name from Date column\n",
        "df[\"year\"] = df['Date'].map(lambda x: x.year).astype(\"object\")     # extract year from Date column and convert it in object type"
      ],
      "metadata": {
        "id": "RgdJRX21zMva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the Date column\n",
        "df.drop(columns=['Date'],inplace=True)"
      ],
      "metadata": {
        "id": "Q8XpsM5gzP-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "NEiVCfi5zg9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = df.select_dtypes(include = 'object')\n",
        "numerical_features = df.select_dtypes(exclude = 'object')"
      ],
      "metadata": {
        "id": "Tpba3fKrHtks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features.head(2)"
      ],
      "metadata": {
        "id": "PzrYQ8EVH0pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features.head(2)"
      ],
      "metadata": {
        "id": "WpbcRL6OH43W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we observe the data we can see that Hour column is a numerical column but it is a time stamp so we have to treat Hour as a categorical feature"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "rentals_by_holiday = df.groupby('Holiday')['Rented Bike Count'].mean()\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "rentals_by_holiday.plot(kind='bar', color=['skyblue', 'lightcoral'])\n",
        "plt.title('Average Bike Rentals on Holidays vs. Non-Holidays')\n",
        "plt.xlabel('Holiday')\n",
        "plt.ylabel('Average Bike Rentals')\n",
        "plt.xticks([0, 1], ['Non-Holiday', 'Holiday'], rotation=0)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is there a difference in bike rentals on holidays vs. non-holidays?"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bar chart compares bike rentals on holidays and non-holidays. In holidays average bike rental increases"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df, x='Hour', y='Rented Bike Count', ci=None)\n",
        "plt.title('Bike Rentals by Hour of the Day')\n",
        "plt.xlabel('Hour')\n",
        "plt.ylabel('Rented Bike Count')\n",
        "plt.xticks(range(24))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the hourly trend in bike rentals?"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line plot illustrates how bike rentals vary by the hour of the day. It reveals peak rental hours and the overall pattern of bike usage throughout the day.\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df, x='Temperature(°C)', y='Rented Bike Count')\n",
        "plt.title('Temperature vs. Bike Rentals')\n",
        "plt.xlabel('Temperature(°C)')\n",
        "plt.ylabel('Rented Bike Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does bike rental demand vary with temperature?"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatterplot examines the relationship between temperature and bike rentals. It helps us understand how temperature influences bike rental demand.\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=df, x='month', y='Rented Bike Count', ci=None)\n",
        "plt.title('Bike Rentals by Month')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Rented Bike Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any seasonal patterns in bike rentals?"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar chart presents bike rentals by month, allowing us to identify any seasonal trends in bike usage.\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=df, x='Seasons', y='Rented Bike Count')\n",
        "plt.title('Bike Rentals by Seasons')\n",
        "plt.xlabel('Seasons')\n",
        "plt.ylabel('Rented Bike Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does weather condition affect bike rentals?"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This box plot visualizes bike rentals across different seasons: Spring, Summer, Autumn, and Winter. It provides insights into how bike rental counts vary with changing seasons."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df, x='Humidity(%)', y='Rented Bike Count')\n",
        "plt.title('Humidity vs. Bike Rentals')\n",
        "plt.xlabel('Humidity(%)')\n",
        "plt.ylabel('Rented Bike Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " How does bike rental demand change with humidity?"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatterplot explores how humidity levels are related to bike rental counts."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df, x='year', y='Rented Bike Count', ci=None)\n",
        "plt.title('Bike Rentals by Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Rented Bike Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How has bike rental demand changed over the years?"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line plot visualizes bike rentals over the years, showing how the demand for bike rentals has evolved"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "rentals_by_day = df.groupby('day_of_week')['Rented Bike Count'].sum()\n",
        "\n",
        "# Define the order of days of the week for proper sorting\n",
        "days_of_week_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "rentals_by_day[days_of_week_order].plot(kind='bar', color='skyblue')\n",
        "plt.title('Total Bike Rentals by Day of the Week')\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Total Bike Rentals')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does the day of the week impact bike rental demand?"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Bar chart showing bike rentals on each day of the week. Highest are on Thursday whereas lowest bike renatls are on Sunday"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "df['Rainy'] = df['Rainfall(mm)'].apply(lambda x: 'Rainy' if x > 0 else 'Non-Rainy')\n",
        "\n",
        "# Group the data by the 'Rainy' column and calculate the mean of 'Rented Bike Count'\n",
        "rainy_vs_non_rainy = df.groupby('Rainy')['Rented Bike Count'].mean().reset_index()\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(rainy_vs_non_rainy['Rainy'], rainy_vs_non_rainy['Rented Bike Count'])\n",
        "plt.title('Bike Rentals on Rainy vs. Non-Rainy Days')\n",
        "plt.xlabel('Day Type')\n",
        "plt.ylabel('Average Bike Rentals')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are bike rentals affected by rain?"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average bike rentals are more on non-rainy days compare to rainy days."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# figure\n",
        "plt.figure(figsize = (20,8))\n",
        "\n",
        "# title\n",
        "plt.suptitle('Univariate Analysis of Categorical Features', fontsize = 20, fontweight = 'bold', y = 1.02)\n",
        "\n",
        "for i, col in enumerate(categorical_features):\n",
        "  # subplots of\n",
        "  plt.subplot(3,3, i+1)\n",
        "\n",
        "  # Countplots\n",
        "  sns.countplot(x = categorical_features[col])\n",
        "\n",
        "  plt.xticks(rotation ='vertical')\n",
        "  plt.title(col)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "klI1N46PGCKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every hour has an equal number of counts in the dataset.\n",
        "\n",
        "Every season has almost equal number of counts.\n",
        "\n",
        "Dataset has more records of No holiday than a holiday which is obvious as most of the days are working days.\n",
        "\n",
        "Dataset has more records of Functioning Day than no functioning day which is obvious as most of the days are working days.\n",
        "\n",
        "Except Friday, other Days have equal number of counts in the dataset.\n",
        "\n",
        "Months like April, June, September, November & February have a slightly low number of count comparted to other months.\n",
        "\n",
        "More data was colected in the year 2018 than 2017."
      ],
      "metadata": {
        "id": "q25_ft4XIu_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rented Bike Count vs. Temperature(°C): There is a positive correlation of approximately 0.54. As temperature increases, bike rentals tend to increase. This is expected as people are more likely to ride bikes in warmer weather.\n",
        "\n",
        "Rented Bike Count vs. Hour: There is a moderate positive correlation of around 0.41. Bike rentals tend to be higher during certain hours, possibly peak commuting times.\n",
        "\n",
        "Rented Bike Count vs. Humidity(%): There is a negative correlation of about -0.20. As humidity increases, bike rentals decrease, although the correlation is relatively weak.\n",
        "\n",
        "Rented Bike Count vs. Wind Speed (m/s): There is a positive correlation of approximately 0.12. Higher wind speeds may be associated with slightly higher bike rentals.\n",
        "\n",
        "Rented Bike Count vs. Dew Point Temperature(°C): There is a positive correlation of about 0.38. As the dew point temperature increases, bike rentals tend to increase.\n",
        "\n",
        "Rented Bike Count vs. Solar Radiation (MJ/m2): There is a positive correlation of approximately 0.26. Bike rentals are influenced by the amount of solar radiation, with more rentals occurring on sunnier days.\n",
        "\n",
        "Rented Bike Count vs. Rainfall(mm) and Snowfall (cm): Both rainfall and snowfall have negative correlations with bike rentals, though the correlations are weak. Rainy or snowy conditions tend to decrease bike rentals, but the effect is not very strong.\n",
        "\n",
        "This"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df)"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.  The average number of rented bikes during holidays is significantly different from the average number of rented bikes on non-holidays.\n",
        "2.   There is a significant difference in the average number of rented bikes on weekdays compared to weekends.\n",
        "3.  The temperature (in °C) has a significant impact on the number of rented bikes.\n",
        "\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): The average number of rented bikes during holidays is equal to the average number of rented bikes on non-holidays. (avg_holiday = avg_non-holiday)\n",
        "\n",
        "Alternative Hypothesis (H1): The average number of rented bikes during holidays is not equal to the average number of rented bikes on non-holidays. (avg_holiday ≠ avg_non-holiday)\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Separating the data into two groups: rented bike counts during holidays and non-holidays\n",
        "bike_counts_holiday = df[df['Holiday'] == 'Holiday']['Rented Bike Count']\n",
        "bike_counts_non_holiday = df[df['Holiday'] == 'No Holiday']['Rented Bike Count']\n",
        "\n",
        "# Perform a two-sample t-test\n",
        "t_stat, p_value = stats.ttest_ind(bike_counts_holiday, bike_counts_non_holiday, equal_var=False)\n",
        "\n",
        "# Set the significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results\n",
        "if p_value < alpha:\n",
        "    print(\"Statement 1: Reject the null hypothesis\")\n",
        "    print(\"Conclusion: The average number of rented bikes during holidays is significantly different from non-holidays.\")\n",
        "else:\n",
        "    print(\"Statement 1: Fail to reject the null hypothesis\")\n",
        "    print(\"Conclusion: There is no significant difference in the average number of rented bikes during holidays and non-holidays.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two-Sample Independent t-Test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, I'm comparing the means of two independent groups (bike counts during holidays and non-holidays).\n",
        "\n",
        "The t-test is appropriate for comparing means of two groups when we want to determine if there is a statistically significant difference between them.\n",
        "\n",
        "We use the \"equal_var=False\" parameter in the t-test because we assume unequal variances between the two groups."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no significant difference in the average number of rented bikes on weekdays compared to weekends. (avg_weekdays = avg_weekends)\n",
        "\n",
        "Alternative Hypothesis (H1): There is a significant difference in the average number of rented bikes on weekdays compared to weekends. (avg_weekdays ≠ avg_weekends)"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Separating the data into two groups: rented bike counts on weekdays and weekends\n",
        "bike_counts_weekdays = df[df['Functioning Day'] == 'Yes']['Rented Bike Count']\n",
        "bike_counts_weekends = df[df['Functioning Day'] == 'No']['Rented Bike Count']\n",
        "\n",
        "# Perform a two-sample t-test\n",
        "t_stat, p_value = stats.ttest_ind(bike_counts_weekdays, bike_counts_weekends, equal_var=False)\n",
        "\n",
        "# Set the significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results\n",
        "if p_value < alpha:\n",
        "    print(\"Statement 2: Reject the null hypothesis\")\n",
        "    print(\"Conclusion: There is a significant difference in the average number of rented bikes on weekdays and weekends.\")\n",
        "else:\n",
        "    print(\"Statement 2: Fail to reject the null hypothesis\")\n",
        "    print(\"Conclusion: There is no significant difference in the average number of rented bikes on weekdays and weekends.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two-Sample Independent t-Test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to Statement 1, we are comparing the means of two independent groups (bike counts on weekdays and weekends).\n",
        "\n",
        "Again, the t-test is suitable for this comparison when we want to test if there is a significant difference in means between the groups."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): The temperature (in C) has no significant impact on the number of rented bikes. (p = 0)\n",
        "\n",
        "Alternative Hypothesis (H1): The temperature (in C) has a significant impact on the number of rented bikes. (p ≠ 0)"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# correlation test between temperature and rented bike counts\n",
        "correlation_coefficient, p_value = stats.pearsonr(df['Temperature(°C)'], df['Rented Bike Count'])\n",
        "\n",
        "# Set the significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results\n",
        "if p_value < alpha:\n",
        "    print(\"Statement 3: Reject the null hypothesis\")\n",
        "    print(\"Conclusion: The temperature has a significant impact on the number of rented bikes.\")\n",
        "else:\n",
        "    print(\"Statement 3: Fail to reject the null hypothesis\")\n",
        "    print(\"Conclusion: There is no significant impact of temperature on the number of rented bikes.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson Correlation Coefficient"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statement 3 involves examining the relationship between two continuous variables (temperature and rented bike counts).\n",
        "\n",
        "The Pearson Correlation Coefficient is used to measure the strength and direction of a linear relationship between two continuous variables.\n",
        "\n",
        "A correlation test is appropriate for this scenario to determine if there is a significant correlation between temperature and bike rentals."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No missing values"
      ],
      "metadata": {
        "id": "V2CRs7yfxX-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a boxplot to detect columns with outliers\n",
        "# figsize\n",
        "plt.figure(figsize = (15,10))\n",
        "\n",
        "# title\n",
        "plt.suptitle('Outlier Analysis of Numeric features', fontsize = 20, fontweight='bold', y=1.02)\n",
        "\n",
        "for index , col in enumerate(numerical_features):\n",
        "  # subplots 3 rows, 3 columns\n",
        "  plt.subplot(4,4, index+1)\n",
        "\n",
        "  # boxplots\n",
        "  sns.boxplot(numerical_features[col])\n",
        "\n",
        "  plt.title(col)\n",
        "  plt.tight_layout()\n",
        ""
      ],
      "metadata": {
        "id": "nNMGqmDeEl7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers are visible in most of the numerical columns.\n",
        "These columns are Rented Bike Count, Wind Speed, Solar Radiation, Rainfall & Snowfall.\n",
        "The columns like Temperature, Humidity, Visibility & Dew point temperature do not contain any outliers."
      ],
      "metadata": {
        "id": "O9eGYk16F5NW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a list of columns that contains outliers\n",
        "outlier_cols = ['Rented Bike Count', 'Wind speed (m/s)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)','Snowfall (cm)']\n",
        "outlier_cols"
      ],
      "metadata": {
        "id": "VzkDaNh4KgKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ranges(data, column):\n",
        "\n",
        "  # Skip categorical columns\n",
        "  if data[column].dtype == 'object':\n",
        "    return None, None\n",
        "  else:\n",
        "    # Calculate quartiles\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "\n",
        "    # Calculate IQR\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Calculate upper and lower ranges\n",
        "    upper_range = Q3 + 1.5 * IQR\n",
        "    lower_range = Q1 - 1.5 * IQR\n",
        "\n",
        "    return upper_range, lower_range"
      ],
      "metadata": {
        "id": "yawn4byfKZ--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_ranges(numerical_features, 'Rented Bike Count')"
      ],
      "metadata": {
        "id": "QYofW9riKj7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Identify potential outliers\n",
        "plt.figure(figsize = (15,10))\n",
        "\n",
        "for index, col in enumerate(outlier_cols):\n",
        "  upper_bound, lower_bound = calculate_ranges(df, col)\n",
        "\n",
        "  # Identify potential outliers\n",
        "  outliers = df[(df[col] > upper_bound) | (df[col] < lower_bound)]\n",
        "\n",
        "  # Visualize the potential outliers\n",
        "  #plt.figure(figsize=(8, 6))\n",
        "\n",
        "  # subplots 3 rows, 3 columns\n",
        "  plt.subplot(3,3, index+1)\n",
        "  plt.hist(df[col], bins=30, color='lightblue', edgecolor='black', label='Data')\n",
        "  plt.hist(outliers[col], bins=10, color='red', edgecolor='black', label='Potential Outliers')\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Frequency')\n",
        "\n",
        "  plt.suptitle('Distribution of Numerical features with Potential Outliers', fontsize = 20, fontweight='bold', y=1.02)\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "  #plt.show()"
      ],
      "metadata": {
        "id": "FVb6EiBBKpm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a function to count the total number of outliers in each column\n",
        "\n",
        "def count_outliers(data):\n",
        "    # Initialize a variable to store the total number of outliers\n",
        "    outlier_count = {}\n",
        "\n",
        "    # Loop through each column in the list containing outliers\n",
        "    for col in outlier_cols:\n",
        "\n",
        "        # Calculate the upper and lower ranges\n",
        "        upper_range, lower_range = calculate_ranges(data, col)\n",
        "\n",
        "        # Count the number of outliers in the column\n",
        "        outlier_count[col] = len(data[(data[col] > upper_range) | (data[col] < lower_range)])\n",
        "\n",
        "    return outlier_count"
      ],
      "metadata": {
        "id": "N69pOB8dL805"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Number of outliers in each column\n",
        "count_outliers(df)"
      ],
      "metadata": {
        "id": "ulHDWo7_MDI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features.columns"
      ],
      "metadata": {
        "id": "GfgPS815MSOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we do not want any transformation in our target variable as it is possible to have outlier in Seoul Environment\n",
        "# Removing rainfall and snowfall as it may remove important information as these 2 columns are highly skewed.\n",
        "num_features = ['Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)']"
      ],
      "metadata": {
        "id": "DEjevpH1MOtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_outliers(df):\n",
        "\n",
        "    for col in num_features:\n",
        "        # Using IQR method to define the range of upper and lower limits\n",
        "        q1 = df[col].quantile(0.25)\n",
        "        q3 = df[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "        # Replacing the outliers with the upper and lower bounds\n",
        "        df[col] = df[col].clip(lower_bound, upper_bound)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "QOb-xhhyMr1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = df.copy()\n",
        "# using the function to treat outliers\n",
        "new_df = clip_outliers(new_df)"
      ],
      "metadata": {
        "id": "Qc3z-7rBM5s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# checking the boxplot after outlier treatment\n",
        "\n",
        "# figsize\n",
        "plt.figure(figsize=(10,6))\n",
        "# title\n",
        "plt.suptitle('Outlier Analysis of Numerical Features', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "for i,col in enumerate(num_features):\n",
        "  # subplot of 3 rows and 2 columns\n",
        "  plt.subplot(3, 2, i+1)\n",
        "\n",
        "  # countplot\n",
        "  sns.boxplot(new_df[col])\n",
        "  # x-axis label\n",
        "  plt.xlabel(col)\n",
        "  plt.tight_layout()\n",
        ""
      ],
      "metadata": {
        "id": "w5R8bYtANFM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for distribution after treating outliers.\n",
        "\n",
        "# figsize\n",
        "plt.figure(figsize=(10,6))\n",
        "# title\n",
        "plt.suptitle('Data Distibution of Numerical Features', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "for i,col in enumerate(num_features):\n",
        "  # subplots 3 rows, 2 columns\n",
        "  plt.subplot(3, 2, i+1)\n",
        "\n",
        "  # dist plots\n",
        "  sns.distplot(new_df[col])\n",
        "  # x-axis label\n",
        "  plt.xlabel(col)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "bysYEPhyNQpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also observe some shifts in the distribution of the data after treating outliers. Some of the data were skewed before handling outliers, but after doing so, the features almost follow the normal distribution. Therefore, we are not utilizing the numerical feature transformation technique."
      ],
      "metadata": {
        "id": "VamcZI_gNXqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "plt.suptitle('Regression Analysis of Numerical features', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "for i, col in enumerate(numerical_features):\n",
        "  plt.subplot(4, 3, i+1)\n",
        "  # regression plots\n",
        "  sns.regplot(x= numerical_features[col], y = numerical_features['Rented Bike Count'], scatter_kws={\"color\": \"purple\"}, line_kws={\"color\": \"yellow\"})\n",
        "\n",
        "  plt.title(f'Dependend variable and {col}')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "aNgmhvIkQ1Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the numerical features are positively correlated to our target variable."
      ],
      "metadata": {
        "id": "-kIj_4m7RD-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Heatmap relative to all numeric columns\n",
        "corr_matrix = df.corr()\n",
        "mask = np.array(corr_matrix)\n",
        "mask[np.tril_indices_from(mask)] = False\n",
        "\n",
        "fig = plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, cbar=True, vmax=0.8, vmin=-0.8, cmap='RdYlGn')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ya-UqDvFRKDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2,4), dpi=100)\n",
        "sns.heatmap(df.corr()[[\"Rented Bike Count\"]].sort_values\n",
        "            (by=\"Rented Bike Count\", ascending=False)[1:],annot=True)\n",
        "plt.title('Features Correlating with Rented Bike Count', fontsize=10, fontweight='bold', y=1.02);"
      ],
      "metadata": {
        "id": "fN8SGvOmRZcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above graph we could see that the columns Temperature and Dew Point Temperature are highly corelated. We can drop one of them. As the corelation between Temperature and our dependent variable \"Bike Rented Count\" is high compared to Dew Point Temperature. So we will Keep the Temperature column and drop the Dew Point Temperature column."
      ],
      "metadata": {
        "id": "L_7VscpIRlse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head(2)"
      ],
      "metadata": {
        "id": "zad01yDqS17J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.drop('Dew point temperature(°C)', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "V2SgAMJ0RrBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.columns"
      ],
      "metadata": {
        "id": "d75pG1K6Q9G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features.columns"
      ],
      "metadata": {
        "id": "HUsR12WOTVAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# droping Year columns as it does not account for any information addition\n",
        "new_df.drop(['year', 'month', 'Rainy','day_of_week'], axis=1, inplace = True)\n",
        "categorical_features.drop(['year', 'month' ,'day_of_week'], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "Cqiye46_UCf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def calculate_vif(X):\n",
        "  vif = pd.DataFrame()\n",
        "  vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "  vif[\"features\"] = X.columns\n",
        "\n",
        "  return vif"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_vif(new_df[[i for i in new_df.describe().columns if i not in ['Rented Bike Count','Date']]])"
      ],
      "metadata": {
        "id": "BG9wfajOSo_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the VIF values, the following columns have relatively low multicollinearity (VIF < 4):\n",
        "\n",
        "Hour, Temperature(°C), Solar Radiation (MJ/m2), Rainfall(mm), Snowfall (cm)"
      ],
      "metadata": {
        "id": "qDMhJiikSxON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each categorical variable.\n",
        "for i in categorical_features:\n",
        "  print(\"Number of unique values in\", i, \"is\" , new_df[i].nunique())"
      ],
      "metadata": {
        "id": "GWFaIxXmUYF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use one hot encoding for Seasons and Numeric encoding for Holiday and Functioning day."
      ],
      "metadata": {
        "id": "YVzkkYFAUf9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = new_df.copy()\n",
        "df1 =pd.get_dummies(df1, columns=['Seasons'],prefix='Seasons')"
      ],
      "metadata": {
        "id": "AUino3q_UpIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head(3)"
      ],
      "metadata": {
        "id": "mcsUxo1lUz2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = pd.get_dummies(new_df, columns = ['Seasons'], prefix='Seasons')"
      ],
      "metadata": {
        "id": "RdtSWOaoX3MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head(2)"
      ],
      "metadata": {
        "id": "eDP3321PX7MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical Encoding for holiday and functioning_day\n",
        "new_df['Holiday'] = new_df['Holiday'].map({'Holiday': 1, 'No Holiday': 0})\n",
        "new_df['Functioning Day'] = new_df['Functioning Day'].map({'Yes': 1, 'No': 0})\n",
        ""
      ],
      "metadata": {
        "id": "A17aWMZ7YEFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head(2)"
      ],
      "metadata": {
        "id": "h2a5dOqnYPA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " We have used one hot encoding for Seasons and Numeric encoding for Holiday and Functioning day"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2 , figsize = (10,4))\n",
        "# Distribution plot of Rented Bike Count\n",
        "dist =sns.distplot(new_df['Rented Bike Count'],hist=True, ax = ax[0])\n",
        "dist.set(xlabel = 'Rented Bike Count', ylabel ='Density', title = 'Distribution Plot of Target Variable')\n",
        "\n",
        "# mean line\n",
        "dist.axvline(new_df['Rented Bike Count'].mean(), color='red', linestyle='dashed', linewidth=2)\n",
        "# median line\n",
        "dist.axvline(new_df['Rented Bike Count'].median(), color='black', linestyle='dashed', linewidth=2)\n",
        "\n",
        "# Boxplot\n",
        "box = sns.boxplot(new_df['Rented Bike Count'], ax= ax[1])\n",
        "box.set(title = 'Outlier Analysis of Target Variable')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph above indicates that the Rented Bike Count has a moderate right skewness. Linear regression assumes that the dependent variable has a normal distribution, therefore, to meet this assumption, we need to take some measures to normalize the distribution.\n",
        "The boxplot above indicates that there are outliers in the rented bike count column."
      ],
      "metadata": {
        "id": "Bt3KBKLyaFS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#apply diffrent tranformation technique and checking data distributation\n",
        "fig,axes = plt.subplots(1,4,figsize=(20,5))\n",
        "sns.distplot((new_df['Rented Bike Count']),ax=axes[0],color='blue').set_title(\" Input data\");\n",
        "\n",
        "# here we use log10\n",
        "#transform only possible in positive value and >0 value so add 0.0000001 in data\n",
        "sns.distplot(np.log1p(new_df['Rented Bike Count']),ax=axes[1],color='black').set_title(\"log1p\");\n",
        "\n",
        "# here we use square root\n",
        "sns.distplot(np.sqrt(new_df['Rented Bike Count']),ax=axes[2], color='magenta').set_title(\"Square root\");\n",
        "\n",
        "# here we use cube root\n",
        "sns.distplot(np.cbrt(new_df['Rented Bike Count']),ax=axes[3], color='red').set_title(\"cube root\");"
      ],
      "metadata": {
        "id": "LLgMNdqmJ8Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying a logarithmic transformation to the dependent variable did not help much as it resulted in a negatively skewed distribution.\n",
        "Square root and cube root transformations were attempted, but they did not result in a normally distributed variable.\n",
        "Therefore, we will use a square root transformation for the regression as it transformed the variable into a well-distributed form."
      ],
      "metadata": {
        "id": "cUVh4OUOLCjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2 , figsize = (12,10))\n",
        "#  checking square root tranformation in our target variable\n",
        "dist =sns.distplot(np.sqrt(new_df['Rented Bike Count']), ax = ax[0])\n",
        "dist.set(xlabel = 'Rented Bike Count', ylabel ='Density', title = 'Distribution Plot of Target Variable in sqrt tranformation')\n",
        "\n",
        "# mean line\n",
        "dist.axvline(np.sqrt(new_df['Rented Bike Count']).mean(), color='red', linestyle='dashed', linewidth=2)\n",
        "# median line\n",
        "dist.axvline(np.sqrt(new_df['Rented Bike Count']).median(), color='black', linestyle='dashed', linewidth=2)\n",
        "\n",
        "# Boxplot\n",
        "box = sns.boxplot(np.sqrt(new_df['Rented Bike Count']), ax= ax[1])\n",
        "box.set(title = 'Outlier Analysis of Target Variable in sqrt tranformation')"
      ],
      "metadata": {
        "id": "YJ44UAtlLLDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By applying the square root transformation to the skewed Rented Bike Count, we were able to obtain an almost normal distribution, which is in line with the general rule that skewed variables should be normalized in linear regression.\n",
        "We found that there are no outliers in the Rented Bike Count column after applying square root transformation."
      ],
      "metadata": {
        "id": "UJiCIWRBLwlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# applying square root on Rented_Bike_Count\n",
        "new_df['Rented Bike Count']=np.sqrt(new_df['Rented Bike Count'])"
      ],
      "metadata": {
        "id": "6qCMS7j2L4qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What manipulations have we done?\n",
        "\n",
        "In our data analysis process, we first examined the relationships between our numerical features and the target variable. We observed that most numerical features had a positive correlation with the target variable, indicating a potential predictive relationship.\n",
        "\n",
        "Upon further investigation using a heatmap and correlation coefficients, we identified a strong correlation (correlation coefficient of 0.91) between 'dew_point_temperature' and 'temperature.' Since 'dew_point_temperature' had a weaker correlation with our target variable, we decided to remove it from our dataset to eliminate redundancy.\n",
        "\n",
        "We also conducted a Variance Inflation Factor (VIF) analysis to address multicollinearity issues. We found that the VIF factor for the 'year' feature was exceptionally high, indicating a high degree of multicollinearity. To mitigate this, we decided to exclude the 'year' feature from our dataset to improve the model's stability.\n",
        "\n",
        "In order to prepare our data for machine learning models, we encoded our categorical features to make them understandable to the model. We employed one-hot encoding for the 'seasons' column, while we used numeric encoding for 'holiday' and 'functioning_day.' Other columns in our dataset were either already encoded or didn't require encoding.\n",
        "\n",
        "Regarding the target variable, we attempted various transformations to make it suitable for regression modeling. We initially tried a logarithmic transformation, but it resulted in a negatively skewed distribution. Subsequently, we experimented with square root and cube root transformations, but they did not lead to a normally distributed variable. Ultimately, we opted for a square root transformation, which successfully transformed the target variable into a more normally distributed form, making it suitable for regression analysis."
      ],
      "metadata": {
        "id": "7pXdpqZmMb2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "#X = independent variable and y = target variable\n",
        "X = new_df.drop('Rented Bike Count', axis=1)\n",
        "y= new_df['Rented Bike Count']"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head(3)"
      ],
      "metadata": {
        "id": "PxqJ3JC4PtFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "HlCEA0wSM1S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data splitting ratio I used in this code is 80% for training data and 20% for testing data. The 80-20 split strikes a reasonable balance between training and testing data. It ensures that the model is not underfitting (due to insufficient training data) or overfitting (due to insufficient testing data)."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        ""
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head(2)"
      ],
      "metadata": {
        "id": "MPeKymVLV7QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Create a Linear Regression model\n",
        "linearRegressor = LinearRegression()\n",
        "\n",
        "# Fit the model to the training data\n",
        "linearRegressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = linearRegressor.predict(X_test)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Calculate Mean Squared Error and R-squared (coefficient of determination)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Linear Regression Evaluation Metrics:\")\n",
        "print(f\"Mean Squared Error : {mse}\")\n",
        "print(f\"R-squared (coefficient of determination): {r2}\")\n",
        "\n",
        "# Optionally, you can also print the model's coefficients and intercept\n",
        "print(\"Coefficients:\", linearRegressor.coef_)\n",
        "print(\"Intercept:\", linearRegressor.intercept_)\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define hyperparameters and their possible values\n",
        "param_grid = {\n",
        "    'fit_intercept': [True, False],  # Whether to calculate the intercept\n",
        "}\n",
        "\n",
        "# Create GridSearchCV instance\n",
        "grid_search = GridSearchCV(linearRegressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the model with hyperparameter tuning\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_lr_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model\n",
        "y_pred = best_lr_model.predict(X_test)\n",
        "\n",
        "# Calculate the MSE and R-squared\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Linear Regression metrics after After Tuning\")\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared (R2):\", r2)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've used GridSearchCV for hyperparameter optimization because it provides a comprehensive search for the best hyperparameter values, which can lead to improved model performance"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The Linear Regression model with hyperparameter tuning did not show significant improvement in terms of evaluation metrics compared to the initial model."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "\n",
        "# Create a Lasso regression model\n",
        "lasso_model = Lasso(alpha=1.0)  # You can adjust the alpha (regularization parameter) as needed\n",
        "\n",
        "# Fit the model to your training data\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = lasso_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "tU-WT2meZ591"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Lasso Regression Evaluation Metrics:\")\n",
        "print(f\"Mean Squared Error : {mse}\")\n",
        "print(f\"R-squared (coefficient of determination): {r2}\")\n",
        "\n",
        "# You can also print the coefficients of the model\n",
        "print(\"Coefficients:\", lasso_model.coef_)\n",
        "print(\"Intercept:\", lasso_model.intercept_)\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "param_dist = {\n",
        "    'alpha': np.logspace(-3, 3, 100),\n",
        "    'max_iter': np.arange(100, 1001, 100)\n",
        "}\n",
        "\n",
        "# Create RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(estimator=lasso_model, param_distributions=param_dist,\n",
        "                                   scoring='neg_mean_squared_error', cv=5, n_jobs=-1,\n",
        "                                   n_iter=50, random_state=42)\n",
        "\n",
        "# Fit the model with hyperparameter tuning\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error and R-squared\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Lasso Regression Evaluation Metrics After Tuning:\")\n",
        "print(f\"Mean Squared Error : {mse}\")\n",
        "print(f\"R-squared (coefficient of determination): {r2}\")\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# You can also print the coefficients of the model\n",
        "print(\"Coefficients:\", best_model.coef_)\n",
        "print(\"Intercept:\", best_model.intercept_)\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used RandomizedSearchCV for hyperparameter optimization in this case."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After hyperparameter tuning, there is a slight improvement in the model's performance. The Mean Squared Error (MSE) decreased from 60.29 to 53.10, and the R-squared (coefficient of determination) increased from 0.6003 to 0.6480. Additionally, the best hyperparameters for the Lasso Regression model were found to be {'max_iter': 500, 'alpha': 0.0107}.\n",
        "\n",
        "Overall, hyperparameter tuning helped improve the model's predictive performance by optimizing the hyperparameters."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Create a Ridge Regression model\n",
        "ridge_model = Ridge(alpha=1.0)  # You can adjust the alpha parameter for regularization\n",
        "\n",
        "# Fit the model on the training data\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ridge_model.predict(X_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Calculate Mean Squared Error and R-squared (coefficient of determination)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Ridge Regression Evaluation Metrics:\")\n",
        "print(f\"Mean Squared Error : {mse}\")\n",
        "print(f\"R-squared (coefficient of determination): {r2}\")\n",
        "\n",
        "# You can also print the coefficients of the model\n",
        "print(\"Coefficients:\", ridge_model.coef_)\n",
        "print(\"Intercept:\", ridge_model.intercept_)"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Define a range of alpha values to search\n",
        "param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=ridge_model, param_grid=param_grid,\n",
        "                           scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the model with hyperparameter tuning\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error and R-squared\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Ridge Regression Evaluation Metrics After Tuning:\")\n",
        "print(f\"Mean Squared Error : {mse}\")\n",
        "print(f\"R-squared (coefficient of determination): {r2}\")\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# You can also print the coefficients of the model\n",
        "print(\"Coefficients:\", best_model.coef_)\n",
        "print(\"Intercept:\", best_model.intercept_)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearch"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the MSE and R-squared values are quite similar between the two cases, indicating that the model's performance did not significantly improve with the optimized alpha. Therefore, in this specific scenario, the choice of alpha value did not have a noticeable impact on the model's performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML Model 4"
      ],
      "metadata": {
        "id": "3c78LSpwqEAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a Decision Tree Regressor model\n",
        "decision_tree_model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = decision_tree_model.predict(X_test)"
      ],
      "metadata": {
        "id": "-OiPuxpxqAyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Calculate R-squared (coefficient of determination)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Decision Tree Regression Evaluation Metrics:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"R-squared (coefficient of determination): {r2}\")"
      ],
      "metadata": {
        "id": "cDVGmJhkq57m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object with the scoring metric (Mean Squared Error)\n",
        "grid_search = GridSearchCV(estimator=decision_tree_model, param_grid=param_grid,\n",
        "                           scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the model with hyperparameter tuning\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Perform cross-validation to evaluate the model\n",
        "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "mean_cv_score = -cv_scores.mean()\n",
        "\n",
        "# Print the results\n",
        "print(\"Decision Tree Regression Hyperparameter Tuning Results:\")\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Mean Cross-Validation Score (MSE): {mean_cv_score}\")\n",
        "\n",
        "# Fit the best model on the entire training data\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics on the test data\n",
        "print(\"Decision Tree Regression Evaluation Metrics on Test Data:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")"
      ],
      "metadata": {
        "id": "r3Swmy5crig-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mean squared error (MSE) on the test data decreased from 32.99 to 23.90 after hyperparameter tuning, indicating better model performance. Additionally, the mean cross-validation score (MSE) on the training data also decreased from 23.16, showing that the model generalizes better to unseen data."
      ],
      "metadata": {
        "id": "GLFq3xn3s9xl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Expalinability"
      ],
      "metadata": {
        "id": "ZPlEjIDTuC5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting feature importances\n",
        "feature_importances = best_model.feature_importances_\n",
        "\n",
        "# Creating a dictonary\n",
        "importance_dict = {'Feature' : list(X.columns),\n",
        "                   'Feature Importance' : feature_importances}\n",
        "\n",
        "# Creating the dataframe\n",
        "importance = pd.DataFrame(importance_dict)\n",
        "sorting_features = importance.sort_values(by=['Feature Importance'],ascending=False)\n",
        "sorting_features"
      ],
      "metadata": {
        "id": "9PFbJbPOuWEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting feature importance graph\n",
        "plt.figure(figsize=(15,5))\n",
        "bar = sns.barplot(x='Feature Importance', y='Feature', data=sorting_features)\n",
        "bar.set_title('Important Features - Decision Tree')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pjkbUtvLu614"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model-5 Random Forest\n",
        "\n",
        "param_grid = {'n_estimators': [50,80],       # number of trees in the ensemble\n",
        "             'max_depth': [15,20],           # maximum number of levels allowed in each tree.\n",
        "             'min_samples_split': [5,15],    # minimum number of samples necessary in a node to cause node splitting.\n",
        "             'min_samples_leaf': [3,5]}      # minimum number of samples which can be stored in a tree leaf.\n",
        "\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Use GridSearchCV to perform a grid search over the parameter grid\n",
        "grid_search = GridSearchCV(rf, param_grid=param_grid, cv=5, scoring='r2')\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X, y)\n",
        ""
      ],
      "metadata": {
        "id": "dh-Xy8SY0t2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best parameters from the grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_"
      ],
      "metadata": {
        "id": "KV41uIYq081Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred_rf = best_model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) and R-squared\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "# Print the results\n",
        "print(\"Random Forest Regression Evaluation Metrics:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_rf}\")\n",
        "print(f\"R-squared (coefficient of determination): {r2_rf}\")\n",
        "print(f\"Best Hyperparameters: {best_params}\")"
      ],
      "metadata": {
        "id": "geG4h6_26mh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Explainability - Random Forest"
      ],
      "metadata": {
        "id": "3eHcBikm8eBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature importance\n",
        "importances = best_model.feature_importances_\n",
        "\n",
        "# Creating a dictonary\n",
        "importance_dict = {'Feature' : list(X.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "# Creating the dataframe\n",
        "importance = pd.DataFrame(importance_dict)\n",
        "sorting_features = importance.sort_values(by=['Feature Importance'],ascending=False)\n",
        "sorting_features"
      ],
      "metadata": {
        "id": "y8nsocdm8il-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting feature importance graph\n",
        "plt.figure(figsize=(15,5))\n",
        "bar = sns.barplot(x='Feature Importance', y='Feature', data=sorting_features)\n",
        "bar.set_title('Important Features - RANDOM FOREST')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tBno2kBJ8sVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Squared Error (MSE):\n",
        "\n",
        " MSE measures the average squared difference between predicted and actual values. In the context of business impact, MSE is relevant because it quantifies the overall prediction error. Lower MSE indicates better model accuracy, which is critical for tasks like sales forecasting or demand prediction. Minimizing MSE can lead to cost savings and improved resource allocation.\n",
        "\n",
        "\n",
        "R-squared (Coefficient of Determination):\n",
        "\n",
        "R-squared measures the proportion of variance in the target variable explained by the model. A higher R-squared value indicates a better fit. In a business context, R-squared helps assess how well the model captures the variability in the data. A high R-squared suggests that the model can make reliable predictions, which can lead to better decision-making and resource optimization.\n",
        "\n",
        "Best Hyperparameters:\n",
        "\n",
        "Identifying the best hyperparameters through hyperparameter tuning is crucial for improving model performance. The choice of hyperparameters can significantly impact model accuracy and generalization. The best hyperparameters help ensure that the model is well-optimized for the specific business problem, leading to better predictions and business outcomes.\n",
        "In summary, these evaluation metrics were considered because they provide insights into the accuracy, goodness of fit, and hyperparameter optimization of the Ridge Regression model. By optimizing these aspects, businesses can make more accurate predictions, reduce errors, and allocate resources efficiently, ultimately leading to a positive business impact."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among the models I created, the Decision Tree Regression model is chosen as the final prediction model for the following reasons:\n",
        "\n",
        "Decision Tree Regression Model:\n",
        "\n",
        "Mean Squared Error (MSE): After hyperparameter tuning, the Decision Tree Regression model achieved an MSE of 23.90 on the test data, which is lower than the MSE of other models like Linear Regression, Lasso Regression, and Ridge Regression. This indicates that the Decision Tree model's predictions are closer to the actual values.\n",
        "\n",
        "R-squared (Coefficient of Determination): The R-squared value of the Decision Tree model is 0.7813, which means that the model explains approximately 78.13% of the variance in the target variable, Rented Bike Count.\n",
        "\n",
        "Hyperparameter Tuning: The Decision Tree model was fine-tuned using hyperparameters such as max_depth, min_samples_leaf, and min_samples_split, resulting in improved performance.\n",
        "\n",
        "Cross-Validation Score: The mean cross-validation score (MSE) of 23.16 indicates that the model generalizes well to unseen data.\n",
        "\n",
        "Interpretability: Decision Tree models are highly interpretable, making it easier to understand the relationship between features and the target variable."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The Decision Tree model  explainability\n",
        "\n",
        " Hour and Temperature(°C) are the two most important features for predicting bike rentals, with nearly equal importance. Functioning Day is the third most important feature, indicating whether the day is a functioning day or not has a significant impact on bike rentals. Rainfall(mm) is also an important feature, suggesting that rainy conditions negatively affect bike rentals.\n",
        " Season-related features such as Seasons_Winter, Seasons_Autumn, Seasons_Spring, and Seasons_Summer have varying degrees of importance, with Winter being the most important among them.\n",
        "\n",
        "\n",
        "\n",
        " Random Forest model explainability\n",
        "\n",
        " Temperature and the hour of the day are the most influential factors in predicting bike rental counts. Other weather-related features like humidity, rainfall, and solar radiation also play significant roles. Additionally, seasonal factors and whether it's a functioning day or a holiday contribute to the variability in bike rental demand."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our analysis commenced with an exploratory data analysis (EDA) of our datasets. Initially, we focused on our target variable, \"Rented Bike Count,\" studying its distribution and relationships with other variables. Subsequently, we delved into categorical and numerical variables, investigating their correlations, distributions, and connections with the target variable. To prepare the data for modeling, we applied one-hot encoding to the categorical variables and identified and removed numerical features with high multicollinearity, which were primarily used for EDA purposes.\n",
        "\n",
        "Following the data preprocessing phase, we proceeded to explore various machine learning models. Our model selection encompassed a spectrum of approaches, including basic Linear Regression and Regularization Models such as Ridge, Lasso,  tree based decision tree as well as more intricate ensemble models like Random Forest. To optimize the predictive performance of our models, we engaged in hyperparameter tuning, fine-tuning the model parameters to achieve the best possible results."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}